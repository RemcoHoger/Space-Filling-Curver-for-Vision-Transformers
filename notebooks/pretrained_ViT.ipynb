{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A notebook to test the zigzag and hilbert embedding on pre-trained ViTs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/remco/.local/lib/python3.10/site-packages/matplotlib/projections/__init__.py:63: UserWarning: Unable to import Axes3D. This may be due to multiple versions of Matplotlib being installed (e.g. as a system package and as a pip package). As a result, the 3D projection is not available.\n",
      "  warnings.warn(\"Unable to import Axes3D. This may be due to multiple versions of \"\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset, Subset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "from fastai.basics import *\n",
    "from timm.loss import SoftTargetCrossEntropy\n",
    "\n",
    "from torchvision.models.vision_transformer import vit_b_16, ViT_B_16_Weights, VisionTransformer\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from src.tokenizers.hilbert_embedding import HilbertEmbedding\n",
    "from src.tokenizers.random_embedding import RandomEmbedding\n",
    "from src.tokenizers.zigzag_embedding import ZigzagEmbedding\n",
    "# from src.models.vit import VisionTransformer\n",
    "from src.models.altvit import SimpleViT, HilbertViT\n",
    "from src.training.scheduler import WarmupCosineScheduler\n",
    "from src.training.train import train_with_mixup, evaluate\n",
    "\n",
    "# Set global variables\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in the pre-trained ViT model and the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VisionTransformer(\n",
      "  (conv_proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
      "  (encoder): Encoder(\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (layers): Sequential(\n",
      "      (encoder_layer_0): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_1): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_2): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_3): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_4): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_5): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_6): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_7): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_8): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_9): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_10): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_11): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  )\n",
      "  (heads): Sequential(\n",
      "    (head): Linear(in_features=768, out_features=257, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "def hilbert_curve(order, size=1.0):\n",
    "    \"\"\"\n",
    "    Generate points for a Hilbert curve of a given order\n",
    "    on the unit square.\n",
    "\n",
    "    Args:\n",
    "        order (int): Recursion depth of the Hilbert curve.\n",
    "        size (float): Length of one side of the entire curve's square.\n",
    "\n",
    "    Returns:\n",
    "        List[Tuple[float, float]]: The list of (x, y) points.\n",
    "    \"\"\"\n",
    "    points = []\n",
    "\n",
    "    def hilbert(x0, y0, xi, xj, yi, yj, n):\n",
    "        if n <= 0:\n",
    "            x = x0 + (xi + yi) / 2\n",
    "            y = y0 + (xj + yj) / 2\n",
    "            points.append((x, y))\n",
    "        else:\n",
    "            hilbert(x0, y0,               yi/2, yj /\n",
    "                    2,               xi/2, xj/2, n-1)\n",
    "            hilbert(x0 + xi/2, y0 + xj/2, xi/2, xj /\n",
    "                    2,               yi/2, yj/2, n-1)\n",
    "            hilbert(x0 + xi/2 + yi/2, y0 + xj/2 +\n",
    "                    yj/2, xi/2, xj/2, yi/2, yj/2, n-1)\n",
    "            hilbert(x0 + xi/2 + yi, y0 + xj/2 + yj, -\n",
    "                    yi/2, -yj/2, -xi/2, -xj/2, n-1)\n",
    "\n",
    "    hilbert(0, 0, size, 0, 0, size, order)\n",
    "    return points\n",
    "\n",
    "def resize_positional_embeddings(model, new_size):\n",
    "    old_posemb = model.encoder.pos_embedding  # [1, 197, D]\n",
    "    cls_token = old_posemb[:, :1, :]\n",
    "    grid = old_posemb[:, 1:, :]  # [1, 196, D]\n",
    "\n",
    "    num_patches_old = int(grid.shape[1] ** 0.5)\n",
    "    grid = grid.reshape(1, num_patches_old, num_patches_old, -1).permute(0, 3, 1, 2)  # [1, D, H, W]\n",
    "    grid = F.interpolate(grid, size=(new_size, new_size), mode='bilinear', align_corners=False)\n",
    "    grid = grid.permute(0, 2, 3, 1).reshape(1, new_size * new_size, -1)\n",
    "    new_posemb = torch.cat([cls_token, grid], dim=1)\n",
    "    model.encoder.pos_embedding = torch.nn.Parameter(new_posemb)\n",
    "\n",
    "def my_forward(self, x):\n",
    "    B, C, H, W = x.shape\n",
    "    D = self.hidden_dim\n",
    "    p = self.patch_size\n",
    "    N = (H//p)*(W//p)\n",
    "\n",
    "    x = self.conv_proj(x)                     # [B,D,H/p,W/p]\n",
    "    x = x.reshape(B, D, N).permute(0,2,1)      # [B,N,D]\n",
    "    x = x[:, self.hilbert_indices, :]         # reorder\n",
    "\n",
    "    cls_tok = self.class_token.expand(B, -1, -1)  # [B,1,D]\n",
    "    x = torch.cat([cls_tok, x], dim=1)            # [B,N+1,D]\n",
    "\n",
    "    x = x + self.pos_embed.unsqueeze(0)           # add your PE\n",
    "\n",
    "    x = self.encoder(x)      # transformer encoder\n",
    "    x = x[:,0]               # take cls\n",
    "    return self.heads(x)\n",
    "\n",
    "# Load model with pretrained weights\n",
    "weights = ViT_B_16_Weights.DEFAULT\n",
    "model = vit_b_16(weights=weights)\n",
    "\n",
    "model.image_size = 128\n",
    "model.patch_size = 16\n",
    "model.num_classes = 257\n",
    "model.channels = 1\n",
    "model.optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=5e-5)\n",
    "model.forward = types.MethodType(my_forward, model)\n",
    "\n",
    "grid_size = (model.image_size // model.patch_size)\n",
    "order     = int(math.log2(model.image_size // model.patch_size))\n",
    "points    = hilbert_curve(order)  # your function returning [(x,y) in [0,1)]\n",
    "flat_idxs = [int(x*grid_size) * grid_size + int(y*grid_size)\n",
    "             for x,y in points]\n",
    "model.register_buffer(\"hilbert_indices\", torch.tensor(flat_idxs, dtype=torch.long))\n",
    "\n",
    "def build_hilbert_pe(indices, dim, T=4, h_param=3.0):\n",
    "    n = indices.numel()\n",
    "    N = int(math.sqrt(n))\n",
    "    pos = indices.to(torch.float32).unsqueeze(1)   # [n,1]\n",
    "    i_ar = torch.arange(dim//2, dtype=torch.float32).unsqueeze(0)\n",
    "    two_pi = 2*math.pi\n",
    "    scale = (2*i_ar * N**2 * pos * two_pi) / (T * n * dim)\n",
    "    phase = (h_param * 2*i_ar * pos * two_pi) / dim\n",
    "    arg   = scale + phase\n",
    "    pe    = torch.cat([torch.sin(arg), torch.cos(arg)], dim=1)\n",
    "    return pe  # [n, dim]\n",
    "\n",
    "pe = build_hilbert_pe(model.hilbert_indices, model.hidden_dim)\n",
    "cls_pe = torch.zeros(1, model.hidden_dim, device=pe.device, dtype=pe.dtype)\n",
    "model.register_buffer(\"pos_embed\", torch.cat([cls_pe, pe], dim=0))  # [N+1, D]\n",
    "# old_conv = model.conv_proj\n",
    "# model.conv_proj = ZigzagEmbedding(\n",
    "#     img_size=model.image_size,\n",
    "#     patch_size=model.patch_size,\n",
    "#     in_channels=3,\n",
    "#     embed_dim=model.hidden_dim\n",
    "# )\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     model.conv_proj.proj.weight.copy_(old_conv.weight)\n",
    "#     model.conv_proj.proj.bias.copy_(old_conv.bias)\n",
    "\n",
    "# Resize pos embeddings for 128x128 input with 16x16 patches = 8x8 = 64 patches\n",
    "resize_positional_embeddings(model, new_size=8)\n",
    "\n",
    "# Replace classifier head\n",
    "model.heads.head = torch.nn.Linear(model.heads.head.in_features, 257)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dataset_name):\n",
    "    \"\"\"Load and return PyTorch-ready datasets.\"\"\"\n",
    "    if dataset_name == 'cifar10':\n",
    "        transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5,), (0.5,))\n",
    "        ])\n",
    "        train_set = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "        test_set = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "        return train_set, test_set\n",
    "\n",
    "    elif dataset_name == 'caltech256':\n",
    "        normalize = transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std =[0.229, 0.224, 0.225]\n",
    "        )\n",
    "\n",
    "        transform_train = transforms.Compose([\n",
    "            transforms.Lambda(lambda img: img.convert(\"RGB\")),\n",
    "            transforms.RandomResizedCrop(128, scale=(0.08, 1.0)),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "            transforms.RandomErasing(p=0.25, scale=(0.02, 0.33), ratio=(0.3, 3.3), value='random')\n",
    "        ])\n",
    "\n",
    "        transform_val = transforms.Compose([\n",
    "            transforms.Lambda(lambda img: img.convert(\"RGB\")),\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(128),\n",
    "            transforms.ToTensor(),\n",
    "            normalize\n",
    "        ])\n",
    "\n",
    "        # 2) Download once (no transform) to extract labels\n",
    "        raw_dataset = datasets.Caltech256(\n",
    "            root=\"./data/caltech256\",\n",
    "            download=True,\n",
    "            transform=None\n",
    "        )\n",
    "        # Caltech256 stores its integer labels in .y\n",
    "        labels = np.array(raw_dataset.y)\n",
    "\n",
    "        # 3) Stratified split\n",
    "        sss = StratifiedShuffleSplit(\n",
    "            n_splits=1, test_size=0.2, random_state=42\n",
    "        )\n",
    "        train_idx, val_idx = next(sss.split(np.zeros(len(labels)), labels))\n",
    "\n",
    "        # 4) Now create two dataset objects with the proper transforms\n",
    "        dataset_train = datasets.Caltech256(\n",
    "            root=\"./data/caltech256\",\n",
    "            download=False,\n",
    "            transform=transform_train\n",
    "        )\n",
    "        dataset_val = datasets.Caltech256(\n",
    "            root=\"./data/caltech256\",\n",
    "            download=False,\n",
    "            transform=transform_val\n",
    "        )\n",
    "\n",
    "        # 5) Wrap in Subset and DataLoader\n",
    "        train_set = Subset(dataset_train, train_idx)\n",
    "        val_set   = Subset(dataset_val,   val_idx)\n",
    "\n",
    "        return train_set, val_set\n",
    "\n",
    "    elif dataset_name == 'imagenette':\n",
    "        path = untar_data(URLs.IMAGENETTE_320)\n",
    "        normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                         std=[0.229, 0.224, 0.225])\n",
    "\n",
    "        transform_train = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(128, scale=(0.08, 1.0)),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "            transforms.RandomErasing(p=0.25, scale=(0.02, 0.33), ratio=(0.3, 3.3), value='random')\n",
    "        ])\n",
    "\n",
    "        transform_val = transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(128),\n",
    "            transforms.ToTensor(),\n",
    "            normalize\n",
    "        ])\n",
    "\n",
    "        train_dataset = datasets.ImageFolder(os.path.join(path, 'train'), transform=transform_train)\n",
    "        val_dataset = datasets.ImageFolder(os.path.join(path, 'val'), transform=transform_val)\n",
    "\n",
    "        return train_dataset, val_dataset\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Dataset '{dataset_name}' not supported.\")\n",
    "\n",
    "train_set, test_set = load_data('caltech256')\n",
    "train_loader = DataLoader(train_set, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=32, shuffle=False)\n",
    "\n",
    "train_set2, test_set2 = load_data('cifar10')\n",
    "train_loader2 = DataLoader(train_set2, batch_size=64, shuffle=True)\n",
    "test_loader2 = DataLoader(test_set2, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_12115/633520665.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_with_mixup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_criterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre_scheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_criterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     print(\n",
      "\u001b[0;32m~/Scriptie/Bachelor_thesis/notebooks/../src/training/train.py\u001b[0m in \u001b[0;36mtrain_with_mixup\u001b[0;34m(model, train_loader, criterion, optimizer, scheduler, device, mixup_alpha)\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         total_correct += (lam * (preds == y_a).float() +\n\u001b[0;32m--> 134\u001b[0;31m                           (1 - lam) * (preds == y_b).float()).sum().item()\n\u001b[0m\u001b[1;32m    135\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mtotal_samples\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "train_criterion = SoftTargetCrossEntropy()\n",
    "test_criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=3e-4)\n",
    "\n",
    "pre_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer, T_max=epochs, eta_min=1e-6\n",
    ")\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer, T_max=epochs * 3, eta_min=1e-6\n",
    ")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss, train_acc = train_with_mixup(model, train_loader, train_criterion, optimizer, pre_scheduler, device)\n",
    "    test_loss, test_acc = evaluate(model, test_loader, test_criterion, device)\n",
    "    print(\n",
    "        f\"Epoch {epoch + 1}: \"\n",
    "        f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, \"\n",
    "        f\"Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}\"\n",
    "    )\n",
    "\n",
    "    # Save checkpoint every 5 epochs\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        checkpoint = {\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'train_loss': train_loss,\n",
    "            'train_acc': train_acc,\n",
    "            'test_loss': test_loss,\n",
    "            'test_acc': test_acc\n",
    "        }\n",
    "        torch.save(checkpoint, f'checkpoint_epoch_{epoch + 1}.pth')\n",
    "        print(f\"Checkpoint saved for epoch {epoch + 1}\")\n",
    "\n",
    "for epoch in range(epochs * 3):\n",
    "    train_loss, train_acc = train_with_mixup(model, train_loader2, train_criterion, optimizer, scheduler, device)\n",
    "    test_loss, test_acc = evaluate(model, test_loader2, test_criterion, device)\n",
    "    print(\n",
    "        f\"Epoch {epoch + 1}: \"\n",
    "        f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, \"\n",
    "        f\"Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
